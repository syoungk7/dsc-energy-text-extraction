{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c69dba2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import json\n",
    "from pdfminer.high_level import extract_text\n",
    "from pdfminer.high_level import extract_pages\n",
    "from pdfminer.layout import LTTextBoxHorizontal\n",
    "import os\n",
    "import requests\n",
    "from urllib.parse import urlparse\n",
    "import numpy as np\n",
    "# import nltk\n",
    "# from nltk.corpus import stopwords\n",
    "# from nltk.stem import WordNetLemmatizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b26fe72",
   "metadata": {},
   "source": [
    "## <h3> Open Metadata </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bcc4fc77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Code</th>\n",
       "      <th>Status</th>\n",
       "      <th>Country Name</th>\n",
       "      <th>Country Code</th>\n",
       "      <th>Category</th>\n",
       "      <th>KeyWord to Search</th>\n",
       "      <th>Title</th>\n",
       "      <th>Exists?</th>\n",
       "      <th>Format</th>\n",
       "      <th>Publication Date</th>\n",
       "      <th>Publication Year</th>\n",
       "      <th>Start Year</th>\n",
       "      <th>End Year</th>\n",
       "      <th>Language</th>\n",
       "      <th>Link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3586</th>\n",
       "      <td>LKA-CPD-2022-EN</td>\n",
       "      <td>Completed</td>\n",
       "      <td>Sri Lanka</td>\n",
       "      <td>LKA</td>\n",
       "      <td>CPD</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Country programme document for Sri Lanka (2023...</td>\n",
       "      <td>Y</td>\n",
       "      <td>Text</td>\n",
       "      <td>29 August–1 September 2022</td>\n",
       "      <td>2022</td>\n",
       "      <td>2023</td>\n",
       "      <td>2027</td>\n",
       "      <td>EN</td>\n",
       "      <td>https://digitallibrary.un.org/record/3982477/f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3587</th>\n",
       "      <td>LKA-CPD-2022-FR</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sri Lanka</td>\n",
       "      <td>LKA</td>\n",
       "      <td>CPD</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Y</td>\n",
       "      <td>Text</td>\n",
       "      <td>29 August–1 September 2022</td>\n",
       "      <td>2022</td>\n",
       "      <td>2023</td>\n",
       "      <td>2027</td>\n",
       "      <td>FR</td>\n",
       "      <td>https://digitallibrary.un.org/record/3982477/f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3588</th>\n",
       "      <td>LKA-CPD-2022-SP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sri Lanka</td>\n",
       "      <td>LKA</td>\n",
       "      <td>CPD</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Y</td>\n",
       "      <td>Text</td>\n",
       "      <td>29 August–1 September 2022</td>\n",
       "      <td>2022</td>\n",
       "      <td>2023</td>\n",
       "      <td>2027</td>\n",
       "      <td>SP</td>\n",
       "      <td>https://digitallibrary.un.org/record/3982477/f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3589</th>\n",
       "      <td>LKA-NEP-2008-EN</td>\n",
       "      <td>Completed</td>\n",
       "      <td>Sri Lanka</td>\n",
       "      <td>LKA</td>\n",
       "      <td>NEP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NATIONAL ENERGY POLICY &amp; STRATEGIES OF SRI LANKA</td>\n",
       "      <td>Y</td>\n",
       "      <td>Text</td>\n",
       "      <td>2008</td>\n",
       "      <td>2008</td>\n",
       "      <td>2008</td>\n",
       "      <td>2017</td>\n",
       "      <td>EN</td>\n",
       "      <td>https://policy.asiapacificenergy.org/sites/def...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3590</th>\n",
       "      <td>LKA-NEP-2019-EN</td>\n",
       "      <td>Completed</td>\n",
       "      <td>Sri Lanka</td>\n",
       "      <td>LKA</td>\n",
       "      <td>NEP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NATIONAL ENERGY POLICY AND STRATEGIES OF SRI L...</td>\n",
       "      <td>Y</td>\n",
       "      <td>Text</td>\n",
       "      <td>2019</td>\n",
       "      <td>2019</td>\n",
       "      <td>2019</td>\n",
       "      <td>2030</td>\n",
       "      <td>EN</td>\n",
       "      <td>https://policy.asiapacificenergy.org/sites/def...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Code     Status Country Name Country Code Category  \\\n",
       "3586  LKA-CPD-2022-EN  Completed    Sri Lanka          LKA      CPD   \n",
       "3587  LKA-CPD-2022-FR        NaN    Sri Lanka          LKA      CPD   \n",
       "3588  LKA-CPD-2022-SP        NaN    Sri Lanka          LKA      CPD   \n",
       "3589  LKA-NEP-2008-EN  Completed    Sri Lanka          LKA      NEP   \n",
       "3590  LKA-NEP-2019-EN  Completed    Sri Lanka          LKA      NEP   \n",
       "\n",
       "     KeyWord to Search                                              Title  \\\n",
       "3586               NaN  Country programme document for Sri Lanka (2023...   \n",
       "3587               NaN                                                NaN   \n",
       "3588               NaN                                                NaN   \n",
       "3589               NaN   NATIONAL ENERGY POLICY & STRATEGIES OF SRI LANKA   \n",
       "3590               NaN  NATIONAL ENERGY POLICY AND STRATEGIES OF SRI L...   \n",
       "\n",
       "     Exists? Format            Publication Date Publication Year Start Year  \\\n",
       "3586       Y   Text  29 August–1 September 2022             2022       2023   \n",
       "3587       Y   Text  29 August–1 September 2022             2022       2023   \n",
       "3588       Y   Text  29 August–1 September 2022             2022       2023   \n",
       "3589       Y   Text                        2008             2008       2008   \n",
       "3590       Y   Text                        2019             2019       2019   \n",
       "\n",
       "     End Year Language                                               Link  \n",
       "3586     2027       EN  https://digitallibrary.un.org/record/3982477/f...  \n",
       "3587     2027       FR  https://digitallibrary.un.org/record/3982477/f...  \n",
       "3588     2027       SP  https://digitallibrary.un.org/record/3982477/f...  \n",
       "3589     2017       EN  https://policy.asiapacificenergy.org/sites/def...  \n",
       "3590     2030       EN  https://policy.asiapacificenergy.org/sites/def...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load Metadata CSV\n",
    "metadata_df = pd.read_excel('../01_Input/00_Metadata/National Energy Document Compilation.xlsx')\n",
    "metadata_df=metadata_df[metadata_df[\"Exists?\"]==\"Y\"]\n",
    "\n",
    "# metadata_df=metadata_df.head(50)\n",
    "#'../01_Input/00_Metadata/seh-document-metadata.csv')\n",
    "\n",
    "#SK: testset\n",
    "metadata_df_test=metadata_df[metadata_df[\"Country Code\"]==\"LKA\"]\n",
    "metadata_df_test.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df116404",
   "metadata": {},
   "source": [
    "## <h3> Donwload PDFs </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56445f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#download a pdf from a url to a path if is exists\n",
    "def download_document(pdf_url,pdf_path):\n",
    "    # Code to download the document from the URL\n",
    "    # Handle different formats (PDF, HTML, etc.)\n",
    "    try:\n",
    "        response = requests.get(pdf_url,timeout=8)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            #content_disposition = response.headers.get(\"content-disposition\")\n",
    "            with open(pdf_path, \"wb\") as file:\n",
    "                file.write(response.content)\n",
    "            print(f\"{pdf_path} successfully downloaded document with path: {pdf_url}\")\n",
    "            return pdf_path\n",
    "        else:\n",
    "            print(f\"Code 200, failed to downloaded document with path: {pdf_url}\")\n",
    "            return \"None\"\n",
    "    except:\n",
    "        print(f\"Failure to download document with path: {pdf_url}\")\n",
    "        return \"None\"\n",
    "#Example Usage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74d2d757",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download missing document\n",
    "def download_document_if_missing(row,pdf_path):\n",
    "\n",
    "    if not os.path.exists(pdf_path):\n",
    "        ##Download document if the pdf is not already present\n",
    "        pdf_url = row['Link']\n",
    "\n",
    "        if not pd.isna(pdf_url):\n",
    "            download_document(pdf_url,pdf_path)\n",
    "        else:\n",
    "            print(pdf_path,\"no pdf or link\")\n",
    "    else:\n",
    "        print(pdf_path,\"already exists\")\n",
    "#Example Usage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e4c7c5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run the download function for all\n",
    "def download_all(metadata_df, missing_only=True):\n",
    "\n",
    "    for index, row in metadata_df.iterrows():\n",
    "        folder=\"../01_Input/01_PDFs/03_National/\"+row[\"Code\"].split(\"-\")[0]\n",
    "\n",
    "        # Check if the directory already exists\n",
    "        if not os.path.exists(folder):\n",
    "            # Create the directory\n",
    "            os.makedirs(folder)\n",
    "            print(folder+\"directory created!\")\n",
    "\n",
    "        pdf_path=folder+'/'+row[\"Code\"]+\".pdf\"\n",
    "        download_document_if_missing(row, pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d26609b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run the download pipeline for all\n",
    "# download_all(metadata_df,missing_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076cf3c6",
   "metadata": {},
   "source": [
    "## <h3> Text Extraction </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3eeaa6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract text from document\n",
    "def extract_text_from_pdf(extract_text_path, pdf_path):\n",
    "\n",
    "    try:\n",
    "        text = extract_text(pdf_path)##imported function\n",
    "        ##export text\n",
    "        with open(extract_text_path, 'w') as file:\n",
    "            file.write(text)\n",
    "    except:\n",
    "        print(\"invalid pdf\",pdf_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5645b574",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pdf = \"../01_Input/01_PDFs/03_National/LKA/LKA-CPD-2022-EN.pdf\"\n",
    "test_pdf2 = \"../01_Input/01_PDFs/03_National/LKA/LKA-NEPro-2022-EN.pdf\"\n",
    "# text = extract_text(test_pdf)\n",
    "test_extract_path = \"../02_Output/00_Extracted-Text/LKA_test.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4e4fdd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_ext = []\n",
    "for page_layout in extract_pages(test_pdf):\n",
    "    for element in page_layout:\n",
    "\n",
    "        if isinstance(element, LTTextBoxHorizontal):\n",
    "            if len(element.get_text().split()) != 1 and '…' not in element.get_text() and '...' not in element.get_text():\n",
    "                text_ext.append(element.get_text().replace('\\n', '').replace('  ', ''))\n",
    "\n",
    "with open(test_extract_path, 'w') as file:\n",
    "    file.write(\"\\n\".join(text_ext))\n",
    "# text_ext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1fcdc39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download missing documents\n",
    "def extract_text_if_missing(extract_text_path, pdf_path):\n",
    "\n",
    "    if not os.path.exists(extract_text_path):\n",
    "        ##extract text if it is not already present\n",
    "        extract_text_from_pdf(extract_text_path, pdf_path)\n",
    "    else:\n",
    "        print(extract_text_path,\"already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "500afc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run the extract function for all\n",
    "def extract_all(metadata_df, missing_only=True):\n",
    "\n",
    "    for index, row in metadata_df.iterrows():\n",
    "        # folder=\"03_National/\"+row[\"Code\"].split(\"-\")[0]## update this for more docs\n",
    "        folder=\"03_National/LKA\"## update this for more docs\n",
    "        pdf_path=\"../01_Input/01_PDFs/\"+folder+'/'+row[\"Code\"]+\".pdf\"\n",
    "        extract_text_folder=\"../02_Output/00_Extracted-Text/\"+folder+'2'\n",
    "\n",
    "        if os.path.exists(pdf_path):\n",
    "\n",
    "            if not os.path.exists(extract_text_folder):\n",
    "                # Create the directory\n",
    "                os.makedirs(extract_text_folder)\n",
    "                print(extract_text_folder+\"directory created!\")\n",
    "\n",
    "            if missing_only:\n",
    "                extract_text_if_missing(extract_text_folder+'/'+row[\"Code\"]+\".txt\", pdf_path)\n",
    "            else:\n",
    "                extract_text_from_pdf(pdf_path)\n",
    "        else:\n",
    "            print(\"can't extract because no pdf\",row[\"Code\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b9a0924d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../02_Output/00_Extracted-Text/03_National/LKA2directory created!\n"
     ]
    }
   ],
   "source": [
    "extract_all(metadata_df_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6418734d",
   "metadata": {},
   "source": [
    "## <h3> Text Cleaning </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "a4983afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Text cleaning \n",
    "\n",
    "def remove_noise(text):\n",
    "    cleaned_text = text.replace('\\uf0a7', ';')\n",
    "    cleaned_text = text.replace('\\r', '\\n')\n",
    "    cleaned_text = re.sub(r\"\\n\", \" \", cleaned_text)  # remove newlines\n",
    "    cleaned_text = re.sub(r\"\\s+\", \" \", cleaned_text)  # replace multiple spaces with a single space\n",
    "    # cleaned_text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", cleaned_text)  # remove non-alphanumeric characters\n",
    "    cleaned_text = re.sub(r\"http\\S+|www\\S+|ftp\\S+\", \"\", cleaned_text) # remove urls\n",
    "    return cleaned_text\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    cleaned_text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    return cleaned_text\n",
    "\n",
    "def remove_stopwords(text): \n",
    "    # nltk.download('stopwords') # only need to run this once\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    filtered_text = [word for word in text.split() if word.lower() not in stop_words]\n",
    "    return \" \".join(filtered_text)\n",
    "\n",
    "def convert_to_lowercase(text): # reduce words to their root forms\n",
    "    return text.lower()\n",
    "\n",
    "\n",
    "\n",
    "# def lemmatize_text(text):\n",
    "#     # nltk.download('wordnet') only need to run this once\n",
    "#     lemmatizer = WordNetLemmatizer() # tokenize the input text into words\n",
    "#     words = nltk.word_tokenize(text) # lemmatize each word and collect the results in a list\n",
    "\n",
    "#     original_words = []\n",
    "#     lemmatized_words = []\n",
    "#     for word in words:\n",
    "#         original_words.append(word)\n",
    "#         lemmatized_word = lemmatizer.lemmatize(word)\n",
    "#         lemmatized_words.append(lemmatized_word)\n",
    "#         # if word != lemmatized_word: # keep track of lemmatized words\n",
    "#         #     print(f\"Word '{word}' changed to '{lemmatized_word}'\")\n",
    "\n",
    "#     lemmatized_text = \" \".join(lemmatized_words)\n",
    "#     return lemmatized_text\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "7cbe32fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(extract_text_path,clean_text_path):\n",
    "    \n",
    "    with open(extract_text_path, 'r') as file:\n",
    "        extract_text=file.read()\n",
    "    \n",
    "    # Remove unwanted characters, whitespace, etc.\n",
    "    # Regular expressions can be helpful here\n",
    "    # Additional cleaning steps as required\n",
    "    \n",
    "    clean_text = remove_noise(extract_text)\n",
    "    # clean_text = remove_punctuation(text)\n",
    "    # clean_text = remove_stopwords(text)\n",
    "    # clean_text = convert_to_lowercase(text)\n",
    "    # clean_text = lemmatize_text(text)\n",
    "    \n",
    "    with open(clean_text_path, 'w') as file:\n",
    "        file.write(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "e1a01c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_if_missing(extract_text_path,clean_text_path):\n",
    "    \n",
    "    if not os.path.exists(clean_text_path):\n",
    "        ##extract text if it is not already present\n",
    "        clean_text(extract_text_path,clean_text_path)\n",
    "\n",
    "    else:\n",
    "        print(clean_text_path,\"already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "26044c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run the clean function for all\n",
    "def clean_all(metadata_df,missing_only=True):\n",
    "    \n",
    "    for index, row in metadata_df.iterrows():\n",
    "\n",
    "        folder=\"03_National/\"+row[\"Code\"].split(\"-\")[0]## update this for more docs\n",
    "        \n",
    "        clean_text_folder=\"../02_Output/01_Cleaned-Text/\"+folder\n",
    "        extract_text_folder=\"../02_Output/00_Extracted-Text/\"+folder\n",
    "        \n",
    "        extract_text_path=extract_text_folder+'/'+row[\"Code\"]+\".txt\"\n",
    "        clean_text_path=clean_text_folder+'/'+row[\"Code\"]+\"-clean.txt\"\n",
    "        \n",
    "        if os.path.exists(extract_text_path):\n",
    "            \n",
    "            if not os.path.exists(clean_text_folder):\n",
    "                # Create the directory\n",
    "                os.makedirs(clean_text_folder)\n",
    "                print(clean_text_folder+\"directory created!\")\n",
    "\n",
    "            if missing_only:\n",
    "                clean_text_if_missing(extract_text_path,clean_text_path)\n",
    "            else:\n",
    "                clean_text(extract_text_path,clean_text_path)\n",
    "        else:\n",
    "            print(\"can't clean because no extract\",row[\"Code\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "e8f1dca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../02_Output/01_Cleaned-Text/03_National/AFG/AFG-CPD-2014-EN-clean.txt already exists\n",
      "../02_Output/01_Cleaned-Text/03_National/AFG/AFG-CPD-2014-FR-clean.txt already exists\n",
      "../02_Output/01_Cleaned-Text/03_National/AFG/AFG-CPD-2014-SP-clean.txt already exists\n",
      "../02_Output/01_Cleaned-Text/03_National/AFG/AFG-NEP-2015-EN-clean.txt already exists\n",
      "../02_Output/01_Cleaned-Text/03_National/AFG/AFG-NREP-41365-EN-clean.txt already exists\n",
      "../02_Output/01_Cleaned-Text/03_National/AFG/AFG-NREP-2015-EN-clean.txt already exists\n",
      "../02_Output/01_Cleaned-Text/03_National/AFG/AFG-NRES-2017-PR-clean.txt already exists\n",
      "../02_Output/01_Cleaned-Text/03_National/AFG/AFG-NRER-2017-EN-clean.txt already exists\n",
      "../02_Output/01_Cleaned-Text/03_National/AFG/AFG-NEPro-2022-EN-clean.txt already exists\n",
      "../02_Output/01_Cleaned-Text/03_National/AFG/AFG-NES-2007-EN-clean.txt already exists\n",
      "../02_Output/01_Cleaned-Text/03_National/AFG/AFG-NEEP-2016-EN-clean.txt already exists\n",
      "../02_Output/01_Cleaned-Text/03_National/AFG/AFG-NRES-2017-FA-clean.txt already exists\n",
      "../02_Output/01_Cleaned-Text/03_National/ALB/ALB-CPD-2021-EN-clean.txt already exists\n",
      "../02_Output/01_Cleaned-Text/03_National/ALB/ALB-CPD-2021-FR-clean.txt already exists\n",
      "../02_Output/01_Cleaned-Text/03_National/ALB/ALB-CPD-2021-SP-clean.txt already exists\n",
      "../02_Output/01_Cleaned-Text/03_National/ALB/ALB-NES-43101-EN-clean.txt already exists\n",
      "../02_Output/01_Cleaned-Text/03_National/ALB/ALB-NEPro-2022-EN-clean.txt already exists\n",
      "../02_Output/01_Cleaned-Text/03_National/ALB/ALB-NEP-2013-EN-clean.txt already exists\n",
      "../02_Output/01_Cleaned-Text/03_National/ALB/ALB-NREP-2021-EN-clean.txt already exists\n",
      "../02_Output/01_Cleaned-Text/03_National/ALB/ALB-NREP-2021-EN-clean.txt already exists\n",
      "../02_Output/01_Cleaned-Text/03_National/ALB/ALB-NECP-44378-EN-clean.txt already exists\n",
      "../02_Output/01_Cleaned-Text/03_National/ALB/ALB-NETS-2019-EN-clean.txt already exists\n",
      "../02_Output/01_Cleaned-Text/03_National/ALB/ALB-NREAP-2016-EN-clean.txt already exists\n",
      "../02_Output/01_Cleaned-Text/03_National/ALB/ALB-NREAP-2018-AL-clean.txt already exists\n",
      "can't clean because no extract ALB-NGP-2016-EN\n",
      "../02_Output/01_Cleaned-Text/03_National/ALB/ALB-NREAP-2015-EN-clean.txt already exists\n",
      "can't clean because no extract ALB-NREDS-2005-EN\n",
      "can't clean because no extract ALB-NREEP-2013-EN\n",
      "../02_Output/01_Cleaned-Text/03_National/DZA/DZA-CPD-2023-EN-clean.txt already exists\n",
      "../02_Output/01_Cleaned-Text/03_National/DZA/DZA-CPD-2023-FR-clean.txt already exists\n",
      "../02_Output/01_Cleaned-Text/03_National/DZA/DZA-CPD-2023-SP-clean.txt already exists\n",
      "can't clean because no extract DZA-NREP-43952-EN\n",
      "../02_Output/01_Cleaned-Text/03_National/DZA/DZA-NEPro-2022-EN-clean.txt already exists\n",
      "../02_Output/01_Cleaned-Text/03_National/AGO/AGO-CPD-2019-EN-clean.txt already exists\n",
      "../02_Output/01_Cleaned-Text/03_National/AGO/AGO-CPD-2019-SP-clean.txt already exists\n",
      "../02_Output/01_Cleaned-Text/03_National/AGO/AGO-CPD-2019-FR-clean.txt already exists\n",
      "can't clean because no extract AGO-NREP-2023-FR\n",
      "../02_Output/01_Cleaned-Text/03_National/AGO/AGO-NEPro-2022-EN-clean.txt already exists\n",
      "can't clean because no extract AGO-SEP-2023-EN\n",
      "../02_Output/01_Cleaned-Text/03_National/ARG/ARG-CPD-2021-EN-clean.txt already exists\n",
      "../02_Output/01_Cleaned-Text/03_National/ARG/ARG-CPD-2021-SP-clean.txt already exists\n",
      "../02_Output/01_Cleaned-Text/03_National/ARG/ARG-NEPro-2022-EN-clean.txt already exists\n",
      "../02_Output/01_Cleaned-Text/03_National/ARM/ARM-CPD-2021-EN-clean.txt already exists\n",
      "../02_Output/01_Cleaned-Text/03_National/ARM/ARM-CPD-2021-SP-clean.txt already exists\n",
      "../02_Output/01_Cleaned-Text/03_National/ARM/ARM-CPD-2021-FR-clean.txt already exists\n",
      "../02_Output/01_Cleaned-Text/03_National/ARM/ARM-NEEP-42205-EN-clean.txt already exists\n",
      "../02_Output/01_Cleaned-Text/03_National/ARM/ARM-NEEP-2010-EN-clean.txt already exists\n",
      "../02_Output/01_Cleaned-Text/03_National/ARM/ARM-LCEDP-2020-EN-clean.txt already exists\n",
      "../02_Output/01_Cleaned-Text/03_National/ARM/ARM-SREP-2014-EN-clean.txt already exists\n",
      "../02_Output/01_Cleaned-Text/03_National/ARM/ARM-NRER-2013-AR-clean.txt already exists\n"
     ]
    }
   ],
   "source": [
    "clean_all(metadata_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbabbc59",
   "metadata": {},
   "source": [
    "<h3> convert final cleaned text and metadata to jsons </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "32e7e7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_document_json(json_path,clean_text_path,row):\n",
    "    \n",
    "    with open(clean_text_path, 'r') as file:\n",
    "        text=file.read()\n",
    "    \n",
    "    document_json = {\n",
    "        'Code': str(row['Code']),\n",
    "        'Status': row['Status'],\n",
    "        'Country Name': row['Country Name'],\n",
    "        'Country Code': str(row['Country Code']),\n",
    "        'Category': row['Category'],\n",
    "        'KeyWord to Search': row['KeyWord to Search'],\n",
    "        'Document Title': row['Title'],\n",
    "        'Exists?': row['Exists?'],\n",
    "        'Category': row['Category'],\n",
    "        'Publication Date': str(row['Publication Date']),\n",
    "        'Publication Year': str(row['Publication Date']),\n",
    "        'Start Year': str(row['Start Year']),\n",
    "        'End Year': str(row['End Year']),\n",
    "        'Language': row['Language'],\n",
    "        'Link': row['Link'],\n",
    "        'Content': text  # Extracted text content from PDF\n",
    "    }\n",
    "\n",
    "\n",
    "    # Write the dictionary to a JSON file\n",
    "    with open(json_path, 'w') as jsonfile:\n",
    "        json.dump(document_json, jsonfile)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "1b95141f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jsonify_if_missing(json_path,clean_text_path,row):\n",
    "    \n",
    "    if not os.path.exists(json_path):\n",
    "        ##extract text if it is not already present\n",
    "        create_document_json(json_path,clean_text_path,row)\n",
    "\n",
    "    else:\n",
    "        print(json_path,\"already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "d127fd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jsonify_all(metadata_df,missing_only=True):\n",
    "    \n",
    "    metadata_df=metadata_df.fillna(np.nan).replace([np.nan], [None])##fixes missing so json works\n",
    "\n",
    "    for index, row in metadata_df.iterrows():\n",
    "\n",
    "        folder=\"03_National/\"+row[\"Code\"].split(\"-\")[0]## update this for more docs\n",
    "        \n",
    "        clean_text_folder=\"../02_Output/01_Cleaned-Text/\"+folder ##later update this to the manual folder\n",
    "        json_folder=\"../02_Output/03_Pdf-Jsons/\"+folder\n",
    "        \n",
    "        clean_text_path=clean_text_folder+'/'+row[\"Code\"]+\"-clean.txt\"\n",
    "        json_path=json_folder+'/'+row[\"Code\"]+\".json\"\n",
    "        \n",
    "        if os.path.exists(clean_text_path):\n",
    "            \n",
    "            if not os.path.exists(json_folder):\n",
    "                # Create the directory\n",
    "                os.makedirs(json_folder)\n",
    "                print(json_folder+\" directory created!\")\n",
    "\n",
    "            if missing_only:\n",
    "                jsonify_if_missing(json_path,clean_text_path,row)\n",
    "            else:\n",
    "                create_document_json(json_path,clean_text_path,row)\n",
    "        else:\n",
    "            print(\"can't jsonify because no clean text\",row[\"Code\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "0f1b8455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../02_Output/03_Pdf-Jsons/03_National/AFG directory created!\n",
      "../02_Output/03_Pdf-Jsons/03_National/ALB directory created!\n",
      "../02_Output/03_Pdf-Jsons/03_National/ALB/ALB-NREP-2021-EN.json already exists\n",
      "can't jsonify because no clean text ALB-NGP-2016-EN\n",
      "can't jsonify because no clean text ALB-NREDS-2005-EN\n",
      "can't jsonify because no clean text ALB-NREEP-2013-EN\n",
      "../02_Output/03_Pdf-Jsons/03_National/DZA directory created!\n",
      "can't jsonify because no clean text DZA-NREP-43952-EN\n",
      "../02_Output/03_Pdf-Jsons/03_National/AGO directory created!\n",
      "can't jsonify because no clean text AGO-NREP-2023-FR\n",
      "can't jsonify because no clean text AGO-SEP-2023-EN\n",
      "../02_Output/03_Pdf-Jsons/03_National/ARG directory created!\n",
      "../02_Output/03_Pdf-Jsons/03_National/ARM directory created!\n"
     ]
    }
   ],
   "source": [
    "jsonify_all(metadata_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2a20a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f13f288",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9546fa78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
